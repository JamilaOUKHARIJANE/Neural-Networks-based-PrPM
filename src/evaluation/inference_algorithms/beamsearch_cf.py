
from __future__ import division
import csv
from pathlib import Path
import pdb
from queue import PriorityQueue
from typing import cast
import distance
import numpy as np
import pandas as pd
import keras
from jellyfish import damerau_levenshtein_distance

from src.commons import shared_variables as shared
from src.commons.log_utils import LogData
from src.evaluation.prepare_data import get_beam_size, encode, get_pn_fitness, compliance_checking
from src.training.train_common import CustomTransformer
import absl.logging
absl.logging.set_verbosity(absl.logging.ERROR)
from tqdm import tqdm


def run_experiments(log_data: LogData, compliant_traces: pd.DataFrame, maxlen, predict_size, char_indices,
                    target_char_indices, target_indices_char, char_indices_group, target_char_indices_group,
                    target_indices_char_group, model_file: Path, output_file: Path, bk_file: Path,
                    method_fitness: str, resource: bool, outcome: bool, weight: list, bk_model):

    # Load model, set this to the model generated by train.py
    model = keras.models.load_model(model_file, custom_objects={'CustomTransformer': CustomTransformer})
    class NodePrediction:
        def __init__(self, crop_trace: pd.DataFrame, probability_of=0):
            self.cropped_trace = crop_trace
            self.cropped_line = ''.join(crop_trace[log_data.act_name_key].tolist())
            if resource:
                self.cropped_line_group = ''.join(crop_trace[log_data.res_name_key].tolist())
            if self.cropped_line[-1] != '!' :
                if resource and self.cropped_line_group == '':
                    self.model_input = encode(crop_trace, log_data, maxlen, char_indices, char_indices_group, resource)
                elif resource and self.cropped_line_group [-1] != '!':
                    self.model_input = encode(crop_trace, log_data, maxlen, char_indices, char_indices_group, resource)

            self.probability_of = probability_of

        def __str__(self):
            return f"Prefix: {self.cropped_line}, prob. {self.probability_of}"

        def __lt__(self, other):
            return -self.probability_of < -other.probability_of
        def get_cropped_trace(self):
            return self.cropped_trace

    class CacheFitness:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, fitness: float):
            self.trace[crop_trace] = fitness
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]

    class CacheTrace:
        def __init__(self):
            self.trace = {}
            
        def add(self, crop_trace:str, output: list):
            self.trace[crop_trace] = output
            
        def get(self, crop_trace:str):
            if crop_trace not in self.trace.keys():
                return None
            else:
                return self.trace[crop_trace]

    def ensure_node_prediction(prefix, log_data, maxlen, char_indices, char_indices_group, resource):
        if isinstance(prefix, NodePrediction):
            return prefix
        else:
            if isinstance(prefix, dict):
                crop_trace = prefix.get('crop_trace', pd.DataFrame())
                probability_of = prefix.get('probability_of', 0)
                return NodePrediction(crop_trace, probability_of)
            elif isinstance(prefix, str):
                crop_trace = pd.DataFrame({log_data.act_name_key: list(prefix)})
                if resource:
                    crop_trace[log_data.res_name_key] = [''] * len(crop_trace)  # Add empty 'Resource' column
                return NodePrediction(crop_trace, 0)
            elif isinstance(prefix, pd.DataFrame):
                return NodePrediction(prefix, 0)
            else:
                raise ValueError(f"Cannot convert {type(prefix)} to NodePrediction")
            
    def apply_trace(trace, prefix_size, log_data, predict_size, bk_file, target_indices_char, target_char_indices,
                    target_indices_char_group, target_char_indices_group, method_fitness, resource, outcome, weight, bk_model):

        if len(trace) > prefix_size:
            
            trace_name = trace[log_data.case_name_key].iloc[0]
            trace_prefix = trace.head(prefix_size)

            # Concatenate activities and resources in the trace prefix
            trace_prefix_act = ''.join(trace_prefix[log_data.act_name_key].tolist())
            trace_prefix_res = ''.join(trace_prefix[log_data.res_name_key].tolist()) if resource else None

            act_prefix = ''.join(trace_prefix[log_data.act_name_key].tolist()) + "_" + str(weight)
            res_prefix = ''.join(trace_prefix[log_data.res_name_key].tolist()) if resource else None
            check_prefix = cache_trace.get(act_prefix+""+res_prefix)
            if check_prefix == None:

                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]

                if resource:
                    res_ground_truth = ''.join(trace_ground_truth[log_data.res_name_key].tolist())
                if outcome:
                    outcome_ground_truth = trace[log_data.label_name_key].iloc[0]

                # Initialize queue for beam search, put root of the tree inside
                visited_nodes: PriorityQueue[NodePrediction] = PriorityQueue()
                visited_nodes.put(NodePrediction(trace_prefix))
                frontier_nodes: PriorityQueue[NodePrediction] = PriorityQueue()

                child_node = None
                record_update = []
                is_violated = False
                for i in range(predict_size - prefix_size): #copia al posto di ground truth
                    if visited_nodes.empty():
                            break
                    violated_nodes = {}
                    for k in range(min(shared.beam_size, len(visited_nodes.queue))):
                        child_node = visited_nodes.get()
                        temp_cropped_trace = child_node.cropped_trace
                        temp_cropped_line = child_node.cropped_line
                        temp_cropped_line_group = child_node.cropped_line_group
                        if child_node.cropped_line[-1] == "!" or child_node.cropped_line_group[-1] == "!":
                            if shared.declare_BK and shared.BK_end:
                                prefix_trace = ensure_node_prediction(temp_cropped_trace, log_data, maxlen,
                                                                  char_indices, char_indices_group,
                                                                  resource)
                                prefix_trace = prefix_trace.cropped_trace if isinstance(prefix_trace,
                                                                                    NodePrediction) else prefix_trace
                                prefix_trace = prefix_trace[:-1]
                                BK_res = compliance_checking(log_data, child_node.cropped_line[-1],
                                                         child_node.cropped_line_group[-1],
                                                         bk_model, prefix_trace)
                            if k == 0:
                                if shared.declare_BK and shared.BK_end and BK_res == np.NINF:  # violated: continue the search
                                    violated_nodes[k] = child_node
                                    continue
                                else: # satisfied or not using BK
                                    visited_nodes = PriorityQueue()
                                    break
                            else:
                                if shared.declare_BK and shared.BK_end:
                                    if BK_res == np.NINF:  # violated: continue the search
                                        if k == min(shared.beam_size, len(visited_nodes.queue)) -1:
                                            visited_nodes = PriorityQueue()
                                            child_node = violated_nodes.get(0)
                                            is_violated = True
                                            break
                                        else:
                                            violated_nodes[k] = child_node
                                            continue
                                    else: # satisfied
                                        if all(violated_nodes.get(i) for i in range(k)):
                                            visited_nodes = PriorityQueue()
                                            break
                                        else:
                                            continue
                                else:
                                    continue
                        enc = child_node.model_input
                        if shared.use_modulator:
                            y = model.predict([enc["x_act"], enc["x_group"]], verbose=0)
                        else:
                            y = model.predict(enc, verbose=0) # make predictions
                        
                        if  not resource and not outcome:
                            pdb.set_trace()
                            y_char = y[0]
                            y_group = None
                            y_o = None
                        elif not resource and outcome:
                            y_char = y[0][0]
                            y_group = None
                            y_o = y[1][0][0]
                        elif resource and not outcome:
                            y_char = y[0][0]
                            y_group = y[1][0]
                            y_o = None
                        elif resource and outcome:
                            y_char = y[0][0]
                            y_group = y[1][0]
                            y_o = y[2][0][0]
                        
                        # combine with fitness
                        if method_fitness is None or weight == 0:
                            fitness = []
                            fitness_temp = []
                            score = y_char
                            
                        else:
                            fitness = [] 
                            fitness_temp = []
                            
                            for f in range(1,len(target_indices_char)+1):# + len(log_data.new_chars)):
                                
                                if f <= len(target_indices_char):
                                    temp_prediction = target_indices_char[f]               
                                else:
                                    temp_prediction = log_data.new_chars[f-len(target_indices_char)]
                                    
                                predicted_row = temp_cropped_trace.tail(1).copy()
                                predicted_row.loc[:, log_data.act_name_key] = temp_prediction
                                temp_cropped_trace_next= pd.concat([temp_cropped_trace, predicted_row])                        
                                temp_cropped_line_next = ''.join(temp_cropped_trace_next[log_data.act_name_key].tolist()) 
                                                                
                                check_cache = cache_fitness.get(temp_cropped_line_next )
                                if check_cache == None:
                                    fitness_current = get_pn_fitness(bk_file, method_fitness, temp_cropped_trace_next, log_data)[trace_name]
                                    cache_fitness.add(temp_cropped_line_next, fitness_current)
                                else:
                                    fitness_current = check_cache
                                    
                                fitness = fitness +  [np.exp(fitness_current) ]
                                fitness_temp = fitness_temp +  [fitness_current]
                                                            
                            if sum(fitness) > 0:
                                fitness = [f/sum(fitness) for f in fitness] 
                            else:
                                fitness = np.repeat(1/len(fitness),len(fitness)).tolist()
                                fitness_temp = np.repeat(1/len(fitness_temp),len(fitness_temp)).tolist()
                                
                            '''if len(log_data.new_chars) > 0:
                                y_char = y_char + min(y_char)*len(log_data.new_chars)'''
                            score = [pow(a,1-weight)*pow(b,weight) for a,b in zip(y_char, fitness)]
                        # Convert temp_cropped_line and temp_cropped_trace to NodePrediction if necessary
                        #temp_cropped_line = ensure_node_prediction(temp_cropped_line, log_data, maxlen, char_indices, char_indices_group, resource)
                        temp_cropped_trace = ensure_node_prediction(temp_cropped_trace, log_data, maxlen, char_indices, char_indices_group, resource)
                         # put top 3 based on score
                        temp_cropped_trace_df = temp_cropped_trace.get_cropped_trace()  # Extract the DataFrame

                        frontier_nodes, record = get_beam_size(frontier_nodes, NodePrediction, child_node, bk_model,
                                                     temp_cropped_trace, temp_cropped_trace_df, score,
                                                               y_group, y_char, fitness_temp, act_ground_truth_org,
                                                    char_indices, target_indices_char, target_char_indices,
                                                    target_indices_char_group, target_char_indices_group, i,
                                                    log_data, resource, beam_size = shared.beam_size)
                        
                        record_update = record_update + record
                        
                    visited_nodes = frontier_nodes
                    frontier_nodes = PriorityQueue()

                predicted = child_node.cropped_line[prefix_size:-1]
                if resource:
                    predicted_group = child_node.cropped_line_group[prefix_size:-1]
                
                if outcome:
                    predicted_outcome = '1' if y_o >= 0.5 else '0'

                act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                output = []
                if len(act_ground_truth) > 0:
                    output.append(trace_name)
                    #output.append(log_data.case_to_variant[trace_name])
                   # output.append( list(log_data.case_to_variant.values()).count(log_data.case_to_variant[trace_name])  )
                    #output.append( list(log_data.case_to_variant_train.values()).count(log_data.case_to_variant_train[trace_name])  )
                    #output.append( list(log_data.case_to_variant_test.values()).count(log_data.case_to_variant_test[trace_name])  )
                    output.append(prefix_size)
                    output.append(trace_prefix_act)
                    output.append(act_ground_truth)
                    output.append(predicted)
                    dls = 1 - \
                        (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                    if dls < 0:
                        dls = 0
                    output.append(dls)
                    output.append(1 - distance.jaccard(predicted, act_ground_truth))
                    
                    if resource:
                        output.append(trace_prefix_res)
                        output.append(res_ground_truth)
                        output.append(predicted_group)
                        dls_res = 1 - \
                            (damerau_levenshtein_distance(predicted_group, res_ground_truth)
                                / max(len(predicted_group), len(res_ground_truth)))
                        if dls_res < 0:
                            dls_res = 0
                        output.append(dls_res)
                        output.append(1 - distance.jaccard(predicted_group, res_ground_truth))
                        # Combine activity and resource strings for combined evaluation
                        combined_ground_truth = ''.join([a + r for a, r in zip(act_ground_truth, res_ground_truth)])
                        combined_predicted = ''.join([a + r for a, r in zip(predicted, predicted_group)])

                        dls_combined = 1 - (
                                    damerau_levenshtein_distance(combined_predicted, combined_ground_truth) / max(
                                len(combined_predicted), len(combined_ground_truth)))
                        if dls_combined < 0:
                            dls_combined = 0
                        output.append(dls_combined)
                    
                    if outcome:
                        output.append(outcome_ground_truth)
                        output.append(predicted_outcome)
                        output.append('1' if outcome_ground_truth == predicted_outcome else '0')

                    #output.append(weight)
                    output.append(' '.join(record_update))
                    output.append('Violated' if is_violated else 'Satisfied')
                    #output.append('>>'.join(map(str, act_ground_truth_org)))
                    
                    #prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                    #output.append('>>'.join(map(str, prediction_org)))
                    cache_trace.add(act_prefix+""+res_prefix, output)
            else:
                print('check_prefix:', check_prefix)
                trace_ground_truth = trace.tail(trace.shape[0] - prefix_size)
                act_ground_truth = ''.join(trace_ground_truth[log_data.act_name_key].tolist())
                output = []
                
                output.append(trace_name)
                #output.append(log_data.case_to_variant[trace_name])
                #output.append( list(log_data.case_to_variant.values()).count(log_data.case_to_variant[trace_name])  )
                #output.append( list(log_data.case_to_variant_train.values()).count(log_data.case_to_variant_train[trace_name])  )
                #output.append( list(log_data.case_to_variant_test.values()).count(log_data.case_to_variant_test[trace_name])  )
                output.append(prefix_size)
                output.append(check_prefix[2]) #6
                output.append(act_ground_truth)
                predicted = check_prefix[4] #8
                output.append(predicted)
                
                dls = 1 - \
                    (damerau_levenshtein_distance(predicted, act_ground_truth) / max(len(predicted), len(act_ground_truth)))
                if dls < 0:
                    dls = 0
                output.append(dls)
                output.append(1 - distance.jaccard(predicted, act_ground_truth))
                #output = output + check_prefix[11:(len(check_prefix)-2)]
                if resource:
                    trace_prefix_res = ''.join(trace_prefix[log_data.res_name_key].tolist())
                    res_ground_truth = ''.join(trace_ground_truth[log_data.res_name_key].tolist())
                    predicted_group = check_prefix[9] #13

                    output.append(trace_prefix_res)
                    output.append(res_ground_truth)
                    output.append(predicted_group)

                    dls_res = 1 - (damerau_levenshtein_distance(predicted_group, res_ground_truth) / max(len(predicted_group), len(res_ground_truth)))
                    dls_res = max(dls_res, 0)  # Ensure non-negative
                    output.append(dls_res)
                    output.append(1 - distance.jaccard(predicted_group, res_ground_truth))
                    # Combine activity and resource strings for combined evaluation
                    combined_ground_truth = ''.join([a + r for a, r in zip(act_ground_truth, res_ground_truth)])
                    combined_predicted = ''.join([a + r for a, r in zip(predicted, predicted_group)])

                    dls_combined = 1 - (
                            damerau_levenshtein_distance(combined_predicted, combined_ground_truth) / max(
                        len(combined_predicted), len(combined_ground_truth)))
                    if dls_combined < 0:
                        dls_combined = 0
                    output.append(dls_combined)
                output.append(check_prefix[13]) #record fitness
                output.append(check_prefix[14]) #compliance satisfied or violated
                #output = output + check_prefix[15:(len(check_prefix) - 2)]
                #act_ground_truth_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in act_ground_truth]
                #output.append('>>'.join(map(str, act_ground_truth_org)))
                
                #prediction_org = [log_data.act_enc_mapping[i] if i != "!" else "" for i in predicted]
                ##output.append('>>'.join(map(str, prediction_org)))
                

            if output:
                with open(output_file, 'a', encoding='utf-8', newline='') as csvfile:
                    spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
                    spamwriter.writerow(output)


##############################################################

    with open(output_file, 'w', encoding='utf-8', newline='') as csvfile:
        spamwriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
        # Headers for the new file
        if not resource and not outcome:
            #spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Trace Prefix Act", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard", "Weight", "Record", "Record_org", "Prediction_org"])
            spamwriter.writerow(["Case ID", "Prefix length","Trace Prefix Act", "Ground truth", "Predicted",
                 "Damerau-Levenshtein", "Jaccard", "Weight"])
        elif not resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Trace Prefix Act", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard", "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight", "Record", "Record_org", "Prediction_org"])
        elif resource and not outcome:
            #spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Trace Prefix Act", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard", "Trace Prefix Res", "Ground Truth Group", "Predicted Group", "Damerau-Levenshtein Resource", "Weight", "Record", "Record_org", "Prediction_org"])
            spamwriter.writerow(["Case ID", "Prefix length",
                 "Trace Prefix Act", "Ground truth", "Predicted Acts", "Damerau-Levenshtein Acts", "Jaccard Acts",
                 "Trace Prefix Res", "Ground Truth Resources", "Predicted Resources", "Damerau-Levenshtein Resources",
                 "Jaccard Resources", "Damerau-Levenshtein Combined","Record", "Compliance"])
        elif resource and outcome:
            spamwriter.writerow(["Case ID", "Variant ID", "Variant_size", "Variant_size_train", "Variant_size_test", "Prefix length", "Trace Prefix Act", "Ground truth", "Predicted", "Damerau-Levenshtein", "Jaccard", "Trace Prefix Res", "Ground Truth Group", "Predicted Group", "Damerau-Levenshtein Resource", "Ground truth outcome", "Predicted outcome", "Outcome diff.", "Weight", "Record", "Record_org", "Prediction_org"])
            
    cache_fitness = CacheFitness()
    cache_trace = CacheTrace()
    for prefix_size in range(log_data.evaluation_prefix_start, log_data.evaluation_prefix_end+1):
        print(prefix_size)
        compliant_traces = compliant_traces.reset_index(drop=True) 
        for w in weight: 
            tqdm.pandas()
            compliant_traces.groupby(log_data.case_name_key).progress_apply(lambda x: apply_trace(x, prefix_size, log_data,
                                                                                    predict_size, bk_file,
                                                                                   target_indices_char, target_char_indices,
                                                                                    target_indices_char_group, target_char_indices_group,
                                                                                    method_fitness, resource, outcome, w, bk_model))
